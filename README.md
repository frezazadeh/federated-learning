# ğŸ›ï¸ A Professional Federated Learning Framework

![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)
![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)
![PyYAML](https://img.shields.io/badge/PyYAML-6.0-red.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)

This repository provides an advanced, object-oriented, and extensible framework for implementing and experimenting with Federated Learning (FL) algorithms in PyTorch. It is designed with professional software engineering principles to serve as a robust foundation for research and development.

## âœ¨ Core Design Principles

This framework is built upon several key design patterns to ensure scalability and maintainability:

1.  **Separation of Concerns**: Cleanly divided modules for data loading, model definition, client logic, server orchestration, and optimizers.

2. **Multi-Algorithm Support**: Switch between FedAvg, FedDANE, FedProx, and FedSGD with a single CLI flag.

3. **Reproducibility**: Fixed random seeds and configurable hyperparameters via a dataclass.

4. **Extensibility**: Add new architectures or optimizers.

## ğŸ“‚ Framework Structure

The framework is organized for maximum clarity and extensibility:


```
federated_learning/
â”œâ”€â”€ config.py           # Hyperparameters & experiment settings
â”œâ”€â”€ data_loader.py      # MNIST download & client partitioning
â”œâ”€â”€ model.py            # CNN architecture (BatchNorm, Dropout)
â”œâ”€â”€ utils.py            # Model & gradient averaging helpers
â”œâ”€â”€ optimizers.py       # Implementations of FedAvg, FedDANE, FedProx, FedSGD
â”œâ”€â”€ client.py           # Encapsulates client-side training logic
â”œâ”€â”€ server.py           # Federated learning orchestration
â””â”€â”€ main.py             # CLI entry point to run experiments           
```


## ğŸ› ï¸ Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/frezazadeh/federated-learning.git
    cd federated-learning
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

Run experiments via the command line using `main.py`:

```bash
python main.py --algo <algorithm_name>
```

### Selecting an Algorithm

Available options for `--algo`:
- `fedavg`
- `feddane`
- `fedprox`
- `fedsgd`

## Algorithms

- **FedAvg:** Clients perform multiple local SGD epochs and the server averages model parameters.  
- **FedProx:** Adds a proximal term Âµâ€–wâˆ’w_globalâ€–Â² to local objectives to limit divergence.  
- **FedDANE:** Combines a Newton-like correction with a proximal term for faster convergence.  
- **FedSGD:** Clients compute gradients on their full local data and the server applies the averaged gradient once per round.

For details, see the respective classes in [`optimizers.py`](optimizers.py) and the orchestration logic in [`server.py`](server.py).


## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

